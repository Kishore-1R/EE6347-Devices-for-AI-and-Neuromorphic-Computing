{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "EE6347 Assignment 5\n",
        "\n",
        "Kishore Rajendran [EE20B064]\n",
        "\n",
        "Part 1 : MNIST Classification using Pytorch"
      ],
      "metadata": {
        "id": "SaYtlb4eNVfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KG6GVc5bAp_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f1b670b-b10f-4ec8-a010-ff59a93085f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aihwkit\n",
            "  Downloading aihwkit-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.1 (from aihwkit)\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from aihwkit) (0.16.0+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.11.3)\n",
            "Requirement already satisfied: requests<3,>=2.25 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.23.5)\n",
            "Collecting protobuf>=4.21.6 (from aihwkit)\n",
            "  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->aihwkit)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->aihwkit) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->aihwkit) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->aihwkit) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->aihwkit)\n",
            "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2023.7.22)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from aihwkit)\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->aihwkit) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->aihwkit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->aihwkit) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=74a334ef0a6c11598e27aa77a7a17513bf075f768b66aec8508822149e355990\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, protobuf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, aihwkit\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aihwkit-0.8.0 lit-17.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 protobuf-4.25.0 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importing Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install aihwkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Getting dataset ready\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Importing the MNIST dataset\n",
        "train_dataset = datasets.MNIST('data/', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST('data/', train=False, transform=transform, download=True)\n",
        "\n",
        "# Creating Data Loaders\n",
        "batch_size = 100\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "hZsWAWnTbgpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0d0835-adae-4117-ffea-1124aa29204f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 95215840.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 21304202.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24014429.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18806050.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the NN architecture using fully connected layers with input sizes matching the MNIST dataset.\n",
        "\n",
        "We use the softmax activation function in the last layer, since it's a multi-class classfifcation problem."
      ],
      "metadata": {
        "id": "JQ5Lqo6xg_Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "model = Net()"
      ],
      "metadata": {
        "id": "KwJ7Gx2wcALO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "g6oXoI0ucCuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using SGD optimizer with Learning rate = 0.01"
      ],
      "metadata": {
        "id": "d5A_hsgiho7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 1000 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "OsRBYcT5cEz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1e3b93-9be7-4ad9-db66-885ed510a8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.311805486679077\n",
            "Epoch 1, Loss: 0.6606583595275879\n",
            "Epoch 2, Loss: 0.3945094347000122\n",
            "Epoch 3, Loss: 0.43225768208503723\n",
            "Epoch 4, Loss: 0.2439851015806198\n",
            "Epoch 5, Loss: 0.22991664707660675\n",
            "Epoch 6, Loss: 0.23388774693012238\n",
            "Epoch 7, Loss: 0.24321424961090088\n",
            "Epoch 8, Loss: 0.22755777835845947\n",
            "Epoch 9, Loss: 0.3205982446670532\n",
            "Epoch 10, Loss: 0.1678895354270935\n",
            "Epoch 11, Loss: 0.16380038857460022\n",
            "Epoch 12, Loss: 0.2488624006509781\n",
            "Epoch 13, Loss: 0.2240212857723236\n",
            "Epoch 14, Loss: 0.12558647990226746\n",
            "Epoch 15, Loss: 0.10311552882194519\n",
            "Epoch 16, Loss: 0.1514860987663269\n",
            "Epoch 17, Loss: 0.13634489476680756\n",
            "Epoch 18, Loss: 0.0776069387793541\n",
            "Epoch 19, Loss: 0.11640062183141708\n",
            "Epoch 20, Loss: 0.07778479158878326\n",
            "Epoch 21, Loss: 0.16256535053253174\n",
            "Epoch 22, Loss: 0.07168073952198029\n",
            "Epoch 23, Loss: 0.13237303495407104\n",
            "Epoch 24, Loss: 0.09695573151111603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating accuracy of this ANN over the MNIST test dataset"
      ],
      "metadata": {
        "id": "eCf0zcfDh3hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss}, Accuracy: {accuracy}%')"
      ],
      "metadata": {
        "id": "VXsm6CxOcgJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa2feb7-55b1-41b2-9a40-5091f5ddcb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.001230151367932558, Accuracy: 96.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 : MNIST Classification using AIHWKIT"
      ],
      "metadata": {
        "id": "wT3rFTjvmVHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same Network definition, but this time using Hardware-aware Analog Linear layers"
      ],
      "metadata": {
        "id": "PQxZDG6CNMi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aihwkit.nn as aihwkit_nn\n",
        "from aihwkit.optim import AnalogSGD\n",
        "from aihwkit.simulator.configs import SingleRPUConfig\n",
        "from aihwkit.simulator.configs.devices import ConstantStepDevice\n",
        "\n",
        "class AnalogNet(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.config =  SingleRPUConfig(device = self.device)\n",
        "        self.fc1 = aihwkit_nn.AnalogLinear(28 * 28, 128, rpu_config = self.config)\n",
        "        self.fc2 = aihwkit_nn.AnalogLinear(128, 64, rpu_config = self.config)\n",
        "        self.fc3 = aihwkit_nn.AnalogLinear(64, 10, rpu_config = self.config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "7X4dM_E4mf0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up all the presets"
      ],
      "metadata": {
        "id": "Ilk64DhLNDjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aihwkit.simulator.presets import ReRamSBPresetDevice, ReRamESPresetDevice,  CapacitorPresetDevice, IdealizedPresetDevice, PCMPresetDevice\n",
        "presets = [ConstantStepDevice(w_min=-0.4), ReRamSBPresetDevice(), ReRamESPresetDevice(), CapacitorPresetDevice(), IdealizedPresetDevice(), PCMPresetDevice()]\n",
        "device_names = [\"ConstantStepDevice\", \"ReRamSBPresetDevice\", \"ReRamESPresetDevice\", \"CapacitorPresetDevice\", \"IdealizedPresetDevice\", \"PCMPresetDevice\"]"
      ],
      "metadata": {
        "id": "U_YW4_I5Mw6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "num_epochs = 10\n",
        "for device in presets:\n",
        "  # Setting up the preset device configs and defining the optimizer\n",
        "  analog_model = AnalogNet(device)\n",
        "  optimizer = AnalogSGD(analog_model.parameters(), lr=0.01, momentum=0.5)\n",
        "  optimizer.regroup_param_groups(analog_model)\n",
        "  print(\"Experimenting with\", device_names[index])\n",
        "  index += 1\n",
        "\n",
        "  # Training loop\n",
        "  analog_model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          output = analog_model(data)\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if batch_idx % 1000 == 0:\n",
        "              print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "  # Test loop\n",
        "  analog_model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          output = analog_model(data)\n",
        "          test_loss += criterion(output, target).item()\n",
        "          pred = output.argmax(dim=1)\n",
        "          correct += pred.eq(target).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  final_an_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "  print(\"Final Validation Accuracy (Hardware-aware):\", final_an_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU5S1E_Nls-N",
        "outputId": "741a3868-be7a-46b9-d713-6ff05bafd4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimenting with ConstantStepDevice\n",
            "Epoch 0, Loss: 2.3017730712890625\n",
            "Epoch 1, Loss: 0.6082190871238708\n",
            "Epoch 2, Loss: 0.2916443347930908\n",
            "Epoch 3, Loss: 0.3476111590862274\n",
            "Epoch 4, Loss: 0.3610057532787323\n",
            "Epoch 5, Loss: 0.42415955662727356\n",
            "Epoch 6, Loss: 0.36414918303489685\n",
            "Epoch 7, Loss: 0.459450900554657\n",
            "Epoch 8, Loss: 0.5076990723609924\n",
            "Epoch 9, Loss: 0.4227752387523651\n",
            "Final Validation Accuracy (Hardware-aware): 81.25\n",
            "Experimenting with ReRamSBPresetDevice\n",
            "Epoch 0, Loss: 4.291841983795166\n",
            "Epoch 1, Loss: 2.313577175140381\n",
            "Epoch 2, Loss: 2.2925162315368652\n",
            "Epoch 3, Loss: 2.2960031032562256\n",
            "Epoch 4, Loss: 2.3212697505950928\n",
            "Epoch 5, Loss: 2.3219447135925293\n",
            "Epoch 6, Loss: 2.300779342651367\n",
            "Epoch 7, Loss: 2.2879981994628906\n",
            "Epoch 8, Loss: 2.3105103969573975\n",
            "Epoch 9, Loss: 2.3013083934783936\n",
            "Final Validation Accuracy (Hardware-aware): 12.44\n",
            "Experimenting with ReRamESPresetDevice\n",
            "Epoch 0, Loss: 4.018839359283447\n",
            "Epoch 1, Loss: 2.312469244003296\n",
            "Epoch 2, Loss: 2.3097116947174072\n",
            "Epoch 3, Loss: 2.3061017990112305\n",
            "Epoch 4, Loss: 2.2965776920318604\n",
            "Epoch 5, Loss: 2.3070130348205566\n",
            "Epoch 6, Loss: 2.299729585647583\n",
            "Epoch 7, Loss: 2.30472993850708\n",
            "Epoch 8, Loss: 2.298504590988159\n",
            "Epoch 9, Loss: 2.3034069538116455\n",
            "Final Validation Accuracy (Hardware-aware): 11.34\n",
            "Experimenting with CapacitorPresetDevice\n",
            "Epoch 0, Loss: 2.2904322147369385\n",
            "Epoch 1, Loss: 0.9007388353347778\n",
            "Epoch 2, Loss: 0.6466345191001892\n",
            "Epoch 3, Loss: 1.148309588432312\n",
            "Epoch 4, Loss: 1.9960606098175049\n",
            "Epoch 5, Loss: 0.8928859233856201\n",
            "Epoch 6, Loss: 1.1951637268066406\n",
            "Epoch 7, Loss: 1.2173601388931274\n",
            "Epoch 8, Loss: 2.0504016876220703\n",
            "Epoch 9, Loss: 1.598048210144043\n",
            "Final Validation Accuracy (Hardware-aware): 74.36\n",
            "Experimenting with IdealizedPresetDevice\n",
            "Epoch 0, Loss: 2.3128576278686523\n",
            "Epoch 1, Loss: 0.7400729656219482\n",
            "Epoch 2, Loss: 0.3946796655654907\n",
            "Epoch 3, Loss: 0.29958251118659973\n",
            "Epoch 4, Loss: 0.3734511137008667\n",
            "Epoch 5, Loss: 0.3370540142059326\n",
            "Epoch 6, Loss: 0.1652500331401825\n",
            "Epoch 7, Loss: 0.33404266834259033\n",
            "Epoch 8, Loss: 0.2087925672531128\n",
            "Epoch 9, Loss: 0.15776203572750092\n",
            "Final Validation Accuracy (Hardware-aware): 90.79\n",
            "Experimenting with PCMPresetDevice\n",
            "Epoch 0, Loss: 2.2958762645721436\n",
            "Epoch 1, Loss: 2.3075790405273438\n",
            "Epoch 2, Loss: 2.3075687885284424\n",
            "Epoch 3, Loss: 2.3038218021392822\n",
            "Epoch 4, Loss: 2.2990705966949463\n",
            "Epoch 5, Loss: 2.290144443511963\n",
            "Epoch 6, Loss: 2.3075406551361084\n",
            "Epoch 7, Loss: 2.297312021255493\n",
            "Epoch 8, Loss: 2.3040666580200195\n",
            "Epoch 9, Loss: 2.3091542720794678\n",
            "Final Validation Accuracy (Hardware-aware): 11.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Validation Accuracies\n",
        "\n",
        "| Device/Model | Accuracy |\n",
        "|-----------------|-----------------|\n",
        "| Pytorch   | 96.39%   |\n",
        "| ConstantStepDevice   | 81.25%   |\n",
        "| ReRamSBPresetDevice  | 12.44%   |\n",
        "| ReRamESPresetDevice   | 11.34%   |\n",
        "| CapacitorPresetDevice   | 74.36%  |\n",
        "| IdealizedPresetDevice   | 90.79%  |\n",
        "| PCMPresetDevice   | 11.35%  |\n",
        "\n",
        "\n",
        "As expected, our ANN has peforms much better when compared to the other Hardware-aware devices.\n",
        "\n",
        "In this assignment, we have succesfully used aihwkit, which is a useful tool that allows us to simulate hardware-aware neural networks and benchmark their performance, to compare the performance of various types of hardware aware neural networks on the popular MNIST classification task."
      ],
      "metadata": {
        "id": "3y2vuqyCaxQA"
      }
    }
  ]
}