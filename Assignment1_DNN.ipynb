{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "_dhDOT0HNfhL",
        "outputId": "eb8104f5-8cba-4f1c-877b-7ea8b9eb0e9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd1aa46b-598c-4367-a502-a1660eb099a5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cd1aa46b-598c-4367-a502-a1660eb099a5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pima-indians-diabetes.csv to pima-indians-diabetes.csv\n"
          ]
        }
      ],
      "source": [
        "## The following method is for uploading the dataset from a local drive. Change if you are uploading from GDrive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "\n",
        "from scipy import ndimage\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "data_load = files.upload()\n",
        "import io\n",
        "data=pd.read_csv(io.BytesIO(data_load['pima-indians-diabetes.csv']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the size of the dataset, i.e. the number of rows and columns"
      ],
      "metadata": {
        "id": "9e1wuDum79mm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4kv0-d2NfhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b144456a-0a43-4602-e0a4-d8c77c71d25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataset = 768\n",
            "Number of columns in the dataset = 9\n"
          ]
        }
      ],
      "source": [
        "# Using data.shape\n",
        "print(\"Number of rows in the dataset = \"+ str(data.shape[0]))\n",
        "print(\"Number of columns in the dataset = \"+ str(data.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, modify the dataset by removing zero values for \"BloodPressure\", \"BMI\" and \"Glucose\"\n",
        "Then define the independent and the dependent variables (x and y)\n",
        "Finally, split the dataset with training and test subsets\n",
        "Check the sizes of the train and the test datasets"
      ],
      "metadata": {
        "id": "A-Sxnm0d9Sn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsim-MKCNfhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdee5b1d-26b3-412a-e784-117456f0ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the processed dataset = 724\n",
            "Number of columns in the processed dataset = 9\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing :  Remove rows with zero values in BloodPressure, BMI, Glucose\n",
        "data_mod = data[(data.BloodPressure!=0) & (data.BMI!=0) & (data.Glucose!=0)]\n",
        "\n",
        "print(\"Number of rows in the processed dataset = \"+ str(data_mod.shape[0]))\n",
        "print(\"Number of columns in the processed dataset = \"+ str(data_mod.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeBmyOyiNfhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272b0d83-218e-4ff7-bda8-5cf2ed9a8a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(579, 8)\n",
            "(579, 1)\n",
            "(145, 8)\n",
            "(145, 1)\n"
          ]
        }
      ],
      "source": [
        "# Use -1 to access last value\n",
        "x = data_mod.iloc[:, :-1]\n",
        "y = data_mod.iloc[:, -1]\n",
        "\n",
        "# Random test-train split using sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=64)\n",
        "# Converting to numpy arrays\n",
        "x_train = x_train.to_numpy()\n",
        "y_train = y_train.to_numpy().reshape(-1,1)\n",
        "x_test = x_test.to_numpy()\n",
        "y_test = y_test.to_numpy().reshape(-1,1)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will define the structure of the network:\n",
        "Create a function that takes x and y as the inputs\n",
        "Use two hidden layers aside from the input and the output layer.\n",
        "The function should return the number of units in the input layer, hidden layer 1, hidden layer 2 and the output layer.\n",
        "Pass the training dataset to check the structure of the network.  "
      ],
      "metadata": {
        "id": "4DeFE4IjAeR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKNOdEpbNfhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa0a69d-bc39-4bc7-cb47-9904a96d4740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8, 50, 50, 1\n"
          ]
        }
      ],
      "source": [
        "## First we will define the structure of the NN:the number of input units, number of hidden units and output units.\n",
        "## Number of input units is equal to the number of features in the dataset\n",
        "ip_units = x.shape[1]\n",
        "## Number of output units is equal to the number of values in outcome\n",
        "op_units = 1\n",
        "## We can choose how many hidden units we want to use\n",
        "h_units1 = 50\n",
        "h_units2 = 50\n",
        "\n",
        "def nn_arch(x, y, h_units1, h_units2):\n",
        "    ip_units = x.shape[1]\n",
        "    op_units = 1\n",
        "    return ip_units, h_units1, h_units2, op_units\n",
        "\n",
        "## Print the number of units in each layer\n",
        "ip_units, h_units1, h_units2, op_units = nn_arch(x, y, h_units1, h_units2)\n",
        "print(str(ip_units)+\", \"+str(h_units1)+\", \"+str(h_units1)+\", \"+str(op_units))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function for parameter initialization.\n",
        "The function should take the units in each layer as inputs.\n",
        "It should return the weights and biases for all the layers.\n",
        "Use random initial weights and zero biases.\n"
      ],
      "metadata": {
        "id": "wc88FfJNXEyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeoiFVsHNfhP"
      },
      "outputs": [],
      "source": [
        "## We then initialize the parameters, i.e the weight and biases for each layer\n",
        "def parameters_initialization(ip_units, h_units1, h_units2, op_units, scaling_factor = 0.04):\n",
        "    h1_weights = np.random.rand(ip_units, h_units1) * scaling_factor\n",
        "    h2_weights = np.random.rand(h_units1, h_units2) * scaling_factor\n",
        "    op_weights = np.random.rand(h_units2, op_units) * scaling_factor\n",
        "    weights = [h1_weights, h2_weights, op_weights]\n",
        "\n",
        "    h1_biases = np.zeros((1,h_units1))\n",
        "    h2_biases = np.zeros((1,h_units2))\n",
        "    op_biases = np.zeros((1,op_units))\n",
        "    biases = [h1_biases, h2_biases, op_biases]\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "# w, b = parameters_initialization(ip_units, h_units1, h_units2, op_units)\n",
        "# print(w[0].shape)\n",
        "# print(w[1].shape)\n",
        "# print(w[2].shape)\n",
        "# print(b[0].shape)\n",
        "# print(b[1].shape)\n",
        "# print(b[2].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the activation functions"
      ],
      "metadata": {
        "id": "iKOwOW-UczbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def sigmoid_diff(z):\n",
        "  return z*(1-z)"
      ],
      "metadata": {
        "id": "AgP865dWc3x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to calculate the forward pass.\n",
        "The function should take input x and the network parameters as inputs\n",
        "The function returns all the \"z\" values and the outputs of each layer in a cache"
      ],
      "metadata": {
        "id": "C64zQyI7c9Zq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKUGV3DXNfhP"
      },
      "outputs": [],
      "source": [
        "## Next we define the forward pass\n",
        "def forward_propagation(x, w, b):\n",
        "    a1 = sigmoid(np.matmul(x, w[0]) + b[0])\n",
        "    a2 = sigmoid(np.matmul(a1, w[1]) + b[1])\n",
        "    a3 = sigmoid(np.matmul(a2, w[2]) + b[2])\n",
        "    a = [a1, a2, a3]\n",
        "    return a\n",
        "\n",
        "\n",
        "#a = forward_propagation(x_train, w, b)\n",
        "#print(a[2].shape)\n",
        "#print(a[1].shape)\n",
        "#print(a[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to calculate the log-loss/cost.\n",
        "The function takes the output of the final layer, y and the parameters as inputs\n",
        "The function returns the calculated cost.\n",
        "Remember that the cost should be calculated over all the training samples."
      ],
      "metadata": {
        "id": "AQfxJPNYdqPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZJBi4fLNfhQ"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_cost(a, y):\n",
        "    # m = Total no. of samples\n",
        "    m = y.shape[0]\n",
        "    loss = -(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "    cost = np.sum(loss)/m\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to calculate the backpropagation.\n",
        "The function takes network parameters, the cache from the function \"forward_propagation\", x and y as inputs.\n",
        "The function should return the gradients, i.e \"dz\", \"dw\" and \"db\" values."
      ],
      "metadata": {
        "id": "Zbgt0sDofYcX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBdRQivGNfhQ"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(x, y, w, a):\n",
        "    m = y.shape[0]\n",
        "    dz3 = a[2] - y\n",
        "    dw3 = np.matmul(a[1].T, dz3)\n",
        "    db3 = np.sum(dz3, axis=0)/m\n",
        "    dz2 = np.matmul(dz3, w[2].T)*sigmoid_diff(a[1])\n",
        "    dw2 = np.matmul(a[0].T, dz2)\n",
        "    db2 = np.sum(dz2, axis=0)/m\n",
        "    dz1 = np.matmul(dz2, w[1].T)*sigmoid_diff(a[0])\n",
        "    dw1 = np.matmul(x.T, dz1)\n",
        "    db1 = np.sum(dz2, axis=0)/m\n",
        "    dw = [dw1, dw2, dw3]\n",
        "    db = [db1, db2, db3]\n",
        "    return dw, db\n",
        "\n",
        "# dw, db = backward_propagation(x_train, y_train, w, a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to update the parameters.\n",
        "The function takes the parameters, the gradients and the learning rate as inputs.\n",
        "The function returns the parameters after updating their values."
      ],
      "metadata": {
        "id": "KNklomhxgT_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5h9Olw8NfhQ"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(lr, w, b, dw, db):\n",
        "    w[0] = w[0] - lr*dw[0]\n",
        "    b[0] = b[0] - lr*db[0]\n",
        "    w[1] = w[1] - lr*dw[1]\n",
        "    b[1] = b[1] - lr*db[1]\n",
        "    w[2] = w[2] - lr*dw[2]\n",
        "    b[2] = b[2] - lr*db[2]\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model using a function.\n",
        "It should take x, y, the hidden units and the number of iterations as inputs.\n",
        "It should return the parameters from the gradient_descent function.\n",
        "Print the cost as a function of the number of iterations."
      ],
      "metadata": {
        "id": "oX28amKRg4jK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkKm3VMPNfhQ"
      },
      "outputs": [],
      "source": [
        "def neural_network_model(x, y, lr, h_units1, h_units2, epoch_num):\n",
        "    ip_units, h_units1, h_units2, op_units = nn_arch(x, y, h_units1, h_units2)\n",
        "    w, b = parameters_initialization(ip_units, h_units1, h_units2, op_units)\n",
        "\n",
        "    for i in range(epoch_num):\n",
        "      a = forward_propagation(x, w, b)\n",
        "      cost = cross_entropy_cost(a[2], y)\n",
        "      dw, db = backward_propagation(x, y, w, a)\n",
        "      w, b = gradient_descent(lr, w, b, dw, db)\n",
        "\n",
        "      print(\"Epoch no.: \"+str(i)+\", Cost = \"+str(cost))\n",
        "\n",
        "    parameters = [w, b]\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_kdJJsXNfhQ"
      },
      "outputs": [],
      "source": [
        "lr = 0.08\n",
        "epoch_num = 100\n",
        "parameters = neural_network_model(x_train, y_train, lr, 50, 50, epoch_num)\n",
        "learned_weights = parameters[0]\n",
        "learned_biases = parameters[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKrcXSTsNfhQ"
      },
      "outputs": [],
      "source": [
        "def model_predict(x, w, b):\n",
        "    op_array = forward_propagation(x, w, b)\n",
        "    op = op_array[2]\n",
        "    y_pred = np.zeros_like(op)\n",
        "    for i in range(op.shape[0]):\n",
        "        if op[i]>=0.5:\n",
        "            y_pred[i] = 1\n",
        "        else:\n",
        "            y_pred[i] = 0\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCmJkSbHNfhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cf9d4a-4b93-4270-ef1f-af14e1f3abb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-53e9c78ddc52>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        }
      ],
      "source": [
        "y_pred_train = model_predict(x_train, learned_weights, learned_biases)\n",
        "y_pred_test = model_predict(x_test, learned_weights, learned_biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo1OpspQNfhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b19099b-1169-4416-83b9-461564dfda36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy = 0.6511226252158895\n",
            "Test Accuracy = 0.6758620689655173\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "print(\"Train Accuracy = \"+str(train_accuracy))\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print(\"Test Accuracy = \"+str(test_accuracy))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}